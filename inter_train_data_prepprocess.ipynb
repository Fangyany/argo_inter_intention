{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "筛选意图预测数据idx，场景：交叉路口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 6399/205942 [05:08<2:40:32, 20.72it/s]\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/data/fyy/new_prediction/argoverse/inter_train_data_prepprocess.ipynb Cell 2\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B4090gpu-zerotier/data/fyy/new_prediction/argoverse/inter_train_data_prepprocess.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m intention_data \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B4090gpu-zerotier/data/fyy/new_prediction/argoverse/inter_train_data_prepprocess.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(avl\u001b[39m.\u001b[39mseq_list))):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B4090gpu-zerotier/data/fyy/new_prediction/argoverse/inter_train_data_prepprocess.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m     av_traj, av_lane, _, city_name \u001b[39m=\u001b[39m get_traj_and_lane(idx)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B4090gpu-zerotier/data/fyy/new_prediction/argoverse/inter_train_data_prepprocess.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=86'>87</a>\u001b[0m     \u001b[39m# 统计这条轨迹的周围路段\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B4090gpu-zerotier/data/fyy/new_prediction/argoverse/inter_train_data_prepprocess.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=87'>88</a>\u001b[0m     traj_start, traj_end \u001b[39m=\u001b[39m av_traj[\u001b[39m0\u001b[39m], av_traj[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[1;32m/data/fyy/new_prediction/argoverse/inter_train_data_prepprocess.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B4090gpu-zerotier/data/fyy/new_prediction/argoverse/inter_train_data_prepprocess.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_traj_and_lane\u001b[39m(idx): \n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B4090gpu-zerotier/data/fyy/new_prediction/argoverse/inter_train_data_prepprocess.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     city_name \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(avl[idx]\u001b[39m.\u001b[39;49mcity)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B4090gpu-zerotier/data/fyy/new_prediction/argoverse/inter_train_data_prepprocess.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     data_seq \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(avl[idx]\u001b[39m.\u001b[39mseq_df)   \u001b[39m# (len, 6)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B4090gpu-zerotier/data/fyy/new_prediction/argoverse/inter_train_data_prepprocess.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     timestamp \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msort(np\u001b[39m.\u001b[39munique(data_seq[\u001b[39m'\u001b[39m\u001b[39mTIMESTAMP\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues))\n",
      "File \u001b[0;32m~/new_prediction/argoverse/argoverse-api/argoverse/data_loading/argoverse_forecasting_loader.py:58\u001b[0m, in \u001b[0;36mArgoverseForecastingLoader.city\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcity\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m     53\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the city name for the current sequence.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m        city name, i.e., either 'PIT' or 'MIA'\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m     _city: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_df[\u001b[39m\"\u001b[39m\u001b[39mCITY_NAME\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m]\n\u001b[1;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m _city\n",
      "File \u001b[0;32m~/new_prediction/argoverse/argoverse-api/argoverse/data_loading/argoverse_forecasting_loader.py:77\u001b[0m, in \u001b[0;36mArgoverseForecastingLoader.seq_df\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mseq_df\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m pd\u001b[39m.\u001b[39mDataFrame:\n\u001b[1;32m     72\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the dataframe for the current sequence.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \n\u001b[1;32m     74\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39m        pandas DataFrame for the current sequence\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     \u001b[39mreturn\u001b[39;00m _read_csv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcurrent_seq)\n",
      "File \u001b[0;32m~/new_prediction/argoverse/argoverse-api/argoverse/data_loading/argoverse_forecasting_loader.py:24\u001b[0m, in \u001b[0;36m_read_csv\u001b[0;34m(path, *args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m@lru_cache\u001b[39m(\u001b[39m128\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_csv\u001b[39m(path: Path, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m pd\u001b[39m.\u001b[39mDataFrame:\n\u001b[1;32m     15\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"A caching CSV reader\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m        pandas DataFrame containing the loaded csv\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39;49mread_csv(path, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/prediction/lib/python3.9/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/prediction/lib/python3.9/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/prediction/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/anaconda3/envs/prediction/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1723\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1720\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1722\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1723\u001b[0m     \u001b[39mreturn\u001b[39;00m mapping[engine](f, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions)\n\u001b[1;32m   1724\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1725\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/prediction/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mif\u001b[39;00m kwds[\u001b[39m\"\u001b[39m\u001b[39mdtype_backend\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[39m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader \u001b[39m=\u001b[39m parsers\u001b[39m.\u001b[39;49mTextReader(src, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munnamed_cols \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[39m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:579\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:668\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "import copy\n",
    "from argoverse.data_loading.argoverse_forecasting_loader import ArgoverseForecastingLoader\n",
    "from argoverse.map_representation.map_api import ArgoverseMap\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import rotate\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from shapely.ops import unary_union\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "am = ArgoverseMap()\n",
    "\n",
    "data_path = '/data/fyy/lanegcn/dataset/dataset/train/data'\n",
    "avl = ArgoverseForecastingLoader(data_path)\n",
    "avl.seq_list = sorted(avl.seq_list)\n",
    "\n",
    "def get_traj_and_lane(idx): \n",
    "    city_name = copy.deepcopy(avl[idx].city)\n",
    "    data_seq = copy.deepcopy(avl[idx].seq_df)   # (len, 6)\n",
    "    timestamp = np.sort(np.unique(data_seq['TIMESTAMP'].values))\n",
    "    \n",
    "    mapping = dict()\n",
    "    for i, ts in enumerate(timestamp):\n",
    "        mapping[ts] = i\n",
    "        \n",
    "    # 某个场景下的所有轨迹，5s内的轨迹，(len, 2)\n",
    "    trajs = np.concatenate((\n",
    "            data_seq.X.to_numpy().reshape(-1, 1),\n",
    "            data_seq.Y.to_numpy().reshape(-1, 1)), 1)\n",
    "\n",
    "    steps = [mapping[x] for x in data_seq['TIMESTAMP'].values]\n",
    "    steps = np.asarray(steps, np.int64)\n",
    "\n",
    "    objs = data_seq.groupby(['TRACK_ID', 'OBJECT_TYPE']).groups\n",
    "    keys = list(objs.keys())\n",
    "    obj_type = [x[1] for x in keys]\n",
    "\n",
    "    av_idx = obj_type.index('AV')  # av_index = 0，获取AV的索引\n",
    "    idcs = objs[keys[av_idx]]\n",
    "\n",
    "    av_traj = trajs[idcs]  # av_traj.shape = (50, 2)\n",
    "    av_step = steps[idcs]  # av_step.shape = (50,)\n",
    "    \n",
    "    # 获取周围车辆的轨迹\n",
    "    del keys[av_idx]\n",
    "    ctx_trajs, ctx_steps = [], []  \n",
    "    for key in keys:\n",
    "        idcs = objs[key]\n",
    "        ctx_trajs.append(trajs[idcs])\n",
    "        ctx_steps.append(steps[idcs])\n",
    "        \n",
    "    data = dict()\n",
    "    data['city'] = city_name\n",
    "    data['trajs'] = [av_traj] + ctx_trajs\n",
    "    data['steps'] = [av_step] + ctx_steps\n",
    "    data['argo_id'] = int(avl.seq_list[idx].name[:-4])\n",
    "    \n",
    "    av_lane = am.get_lane_ids_in_xy_bbox(av_traj[0][0], av_traj[0][1], city_name, 5)\n",
    "    return av_traj, av_lane, timestamp[0], city_name\n",
    "\n",
    "# 读取PIT和MIA的十字路口的lane_id\n",
    "intersection_data_path = [\n",
    "    'intersection_data/PIT/intersection_PIT_id.pickle',\n",
    "    'intersection_data/MIA/intersection_MIA_id.pickle'\n",
    "]\n",
    "intersection_data = {}\n",
    "for path in intersection_data_path:\n",
    "    with open(path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    file_name = path.split('/')[-2]\n",
    "    intersection_data[file_name] = data\n",
    "\n",
    "\n",
    "# 统计意图预测的数据集，前20帧在停止先前，且至少15帧的轨迹在十字路口内（能够标注意图）\n",
    "intention_data = []\n",
    "for idx in tqdm(range(len(avl.seq_list))):\n",
    "    av_traj, av_lane, _, city_name = get_traj_and_lane(idx)\n",
    "    \n",
    "    # 统计这条轨迹的周围路段\n",
    "    traj_start, traj_end = av_traj[0], av_traj[-1]\n",
    "    lane_nearest = am.get_lane_ids_in_xy_bbox(traj_start[0], traj_start[1], city_name, 5)\n",
    "    lane_nearest += am.get_lane_ids_in_xy_bbox(traj_end[0], traj_end[1], city_name, 5)\n",
    "\n",
    "    # 获取交叉路口的多边形\n",
    "    intersection_polygon = []\n",
    "    for lane_id in lane_nearest:\n",
    "        if lane_id in intersection_data[city_name]:\n",
    "            polygon = Polygon(am.get_lane_segment_polygon(lane_id, city_name))\n",
    "            if polygon.is_valid:\n",
    "                intersection_polygon.append(polygon)\n",
    "    merged_polygon_intersection = unary_union(intersection_polygon) if intersection_polygon else None  \n",
    "    \n",
    "    # 获取直行道路的多边形\n",
    "    straight_road_polygon = []\n",
    "    for lane_id in lane_nearest:\n",
    "        if lane_id in intersection_data[city_name]: continue\n",
    "        polygon = Polygon(am.get_lane_segment_polygon(lane_id, city_name))\n",
    "        if polygon.is_valid:\n",
    "            straight_road_polygon.append(polygon)\n",
    "    merged_polygon_straight_road = unary_union(straight_road_polygon) if straight_road_polygon else None\n",
    "           \n",
    "    # 统计轨迹在直行道路内的帧数       \n",
    "    frames_inside_straight = 0\n",
    "    for x, y in av_traj[:20]:\n",
    "        point = Point(x, y)\n",
    "        if merged_polygon_straight_road and merged_polygon_straight_road.contains(point):\n",
    "            frames_inside_straight += 1\n",
    "    \n",
    "    # 统计轨迹在交叉路口内的帧数\n",
    "    frames_inside_intersection = 0\n",
    "    for x, y in av_traj[20:]:\n",
    "        point = Point(x, y)\n",
    "        if merged_polygon_intersection and merged_polygon_intersection.contains(point):\n",
    "            frames_inside_intersection += 1\n",
    "\n",
    "    if frames_inside_straight == 20 and frames_inside_intersection > 15:\n",
    "        intention_data.append(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "意图数据的数量 8540\n"
     ]
    }
   ],
   "source": [
    "file_name = '/data/fyy/new_prediction/argoverse/intersection_data/intention_train_av_idx.pkl'\n",
    "\n",
    "# with open(file_name, 'wb') as file:\n",
    "#     pickle.dump(intention_data, file)\n",
    "    \n",
    "with open(file_name, 'rb') as file:\n",
    "    intention_data = pickle.load(file)\n",
    "\n",
    "print(\"意图数据的数量\", len(intention_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取对应idx轨迹, 生成测试集和训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_direction(traj):\n",
    "    # 计算第20帧的角度\n",
    "    initial_vector = traj[0] - traj[3]\n",
    "    initial_angle = np.arctan2(initial_vector[1], initial_vector[0])\n",
    "\n",
    "    # 计算最后一帧的角度\n",
    "    final_vector = traj[-5] - traj[-1]\n",
    "    final_angle = np.arctan2(final_vector[1], final_vector[0])\n",
    "\n",
    "    # 计算角度差异\n",
    "    angle_threshold = np.pi / 12\n",
    "\n",
    "    angle_diff = final_angle - initial_angle\n",
    "\n",
    "    # 判断是否转向\n",
    "    if abs(angle_diff) > angle_threshold:\n",
    "        if angle_diff > 0:\n",
    "            return [0, 1, 0]\n",
    "        else:\n",
    "            return [0, 0, 1]\n",
    "    else:\n",
    "        return [1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_label = []\n",
    "left, right, through = 0, 0, 0  # 左转 1056, 右转 922, 直行 6562(1000), 总数 8540\n",
    "for idx in intention_data:\n",
    "    av_traj, av_lane, _, city_name = get_traj_and_lane(idx)\n",
    "    label = turn_direction(av_traj[20:])\n",
    "    if label == [1, 0, 0] and through >= 1000:\n",
    "        continue\n",
    "    if label == [0, 1, 0]:\n",
    "        left += 1\n",
    "    elif label == [0, 0, 1]:\n",
    "        right += 1\n",
    "    elif label == [1, 0, 0]:\n",
    "        through += 1\n",
    "    traj_label.append([av_traj[:20], label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "左转 1056\n",
      "右转 922\n",
      "直行 1000\n",
      "总数 2978\n"
     ]
    }
   ],
   "source": [
    "print(\"左转\", left)\n",
    "print(\"右转\", right)\n",
    "print(\"直行\", through)\n",
    "print(\"总数\", len(traj_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '/data/fyy/new_prediction/argoverse/intersection_data/intention_train_av_traj_and_label.pkl'\n",
    "\n",
    "with open(file_name, 'wb') as file:\n",
    "    pickle.dump(traj_label, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
